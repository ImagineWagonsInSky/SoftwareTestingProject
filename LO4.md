# Learning Outcome 4
This document is the Test Evaluation document for PizzaDronzz

## Gaps and Omissions

The testing strategy for PizzaDronz covers a broad range of functional scenarios, including order validation, pathfinding, and error handling. However, there are some notable gaps and omissions. For instance, while the pathfinding logic is tested using GeoJSON export and visual inspection, there is limited automated validation of edge cases such as tightly constrained no-fly zones or overlapping polygons that may trap the pathfinding algorithm. Additionally, the tests do not explicitly verify the optimality of paths generated by the A* algorithm, such as ensuring that the path is the shortest possible given the constraints. This could lead to undetected inefficiencies in certain scenarios. However, we could mostly assume that no-fly zones were mostly simple, so there is no real need to achieve maximum correctness in this requirement.

Integration testing also lacks coverage for scenarios where the REST API server returns inconsistent data. While try-catch blocks handle unreachable services, there is no simulation of specific failure modes, such as partial or delayed responses, to evaluate system resilience. Furthermore, no performance tests exist to assess how the system handles high loads or large datasets, such as a substantial number of orders or highly complex no-fly zone configurations. Incorporating such tests would ensure the application remains robust and performant under diverse real-world conditions.

## Target coverage/performance levels for different testing procedures

The target coverage for the testing procedures should focus on both functional correctness and measurable performance metrics. For functional requirements, a goal should be set to achieve 100% coverage for critical validation logic, such as ensuring all order constraints (e.g., pizza availability, price correctness, and credit card validity) are tested against diverse scenarios, including valid and invalid inputs. This guarantees that every validation path is exercised and handled as expected. Similarly, pathfinding functionality should target comprehensive test coverage for different configurations of no-fly zones, central area constraints, and various starting and goal positions to ensure correctness and adherence to constraints in all possible scenarios.

From a performance perspective, testing should aim to validate that key operations, such as order validation and pathfinding, execute within a reasonable time frame under realistic loads. For instance, the A* pathfinding algorithm should complete calculations within sub-second performance for typical city-scale no-fly zone configurations, while maintaining consistent responsiveness for endpoints under concurrent requests. Additionally, resource-intensive tasks, such as fetching and caching data from external services, should be stress-tested to ensure scalability without exceeding memory or processing thresholds. Setting and monitoring these measurable targets ensures the system performs efficiently and remains robust under real-world conditions.

## Testing comparison with target levels

The testing carried out achieves strong functional coverage for validation logic and pathfinding accuracy but falls slightly short of fully meeting performance and edge-case robustness targets. While validation tests comprehensively cover common scenarios, edge cases like overlapping no-fly zones or invalid external service responses were less rigorously tested. This limits assurance in handling complex or rare scenarios that might occur in production environments.

Performance testing was minimal, focusing mainly on functional correctness rather than execution speed or scalability. For example, while the A* algorithm demonstrated reliable results for smaller cases, stress tests were conducted but were minimal so did not ensure consistent performance with highly complex no-fly zone configurations or large datasets. These gaps highlight areas where additional resources, such as automated performance testing tools and more robust integration tests, would be necessary to meet realistic production-level targets.

## How to achieve target levels

To achieve the target levels, additional automated tests should be developed to address edge cases and performance requirements. For pathfinding, integrating stress tests with complex no-fly zone configurations and larger datasets would ensure scalability and consistent execution times. Tools like JMeter or Gatling could simulate high loads and identify performance bottlenecks in endpoints.

For validation, mock services could simulate partial, delayed, or malformed responses from external systems to test the resilience of REST calls. Adding mutation testing would improve the robustness of validation logic by identifying untested scenarios. Incorporating these enhancements would address current gaps and bring the testing closer to production-level standards.